<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>dvt.keras API documentation</title>
<meta name="description" content="Annotators that require installing keras and tensorflow." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<style>.homelink{display:block;font-size:2em;font-weight:bold;color:#555;padding-bottom:.5em;border-bottom:1px solid silver}.homelink:hover{color:inherit}.homelink img{max-width:20%;max-height:5em;margin:auto;margin-bottom:.3em}</style>
<link rel="canonical" href="https://pdoc3.github.io/pdoc/doc/dvt/keras.html">
<link rel="icon" href="https://pdoc3.github.io/pdoc/logo.png">
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>dvt.keras</code></h1>
</header>
<section id="section-intro">
<p>Annotators that require installing keras and tensorflow.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
&#34;&#34;&#34;Annotators that require installing keras and tensorflow.
&#34;&#34;&#34;

from importlib import import_module

import ssl
from cv2 import resize, cvtColor, COLOR_BGR2RGB
from numpy import float32, expand_dims, uint8
from pandas import DataFrame

from .abstract import ImageAnnotator


class FaceAnnotator(ImageAnnotator):
    &#34;&#34;&#34;Annotator for detecting faces and embedding them as a face vector.&#34;&#34;&#34;

    def __init__(self, detector, embedding=None):
        self.detector = detector
        self.embedding = embedding

    def annotate_image(self, img):
        &#34;&#34;&#34;Annotate the batch of frames with the face annotator.&#34;&#34;&#34;

        img_rgb = cvtColor(img, COLOR_BGR2RGB)
        f_faces = self.detector.detect(img_rgb)

        if self.embedding is not None and f_faces is not None:
            n_face = len(f_faces[&#34;top&#34;])
            f_faces[&#34;embed&#34;] = []
            for i in range(n_face):
                embed = self.embedding.embed(
                    img_rgb,
                    top=f_faces[&#34;top&#34;][i],
                    right=f_faces[&#34;right&#34;][i],
                    bottom=f_faces[&#34;bottom&#34;][i],
                    left=f_faces[&#34;left&#34;][i],
                )
                f_faces[&#34;embed&#34;] += [embed]

        if not f_faces:
            f_faces = DataFrame(
                columns = [&#34;top&#34;, &#34;right&#34;, &#34;bottom&#34;, &#34;left&#34;, &#34;confidence&#34;]
            )

        return {&#34;face&#34;: f_faces}


class FaceDetectMtcnn:
    &#34;&#34;&#34;Detect faces using the Multi-task Cascaded CNN model.

    Attributes:
        cutoff (float): A cutoff value for which faces to include in the final
            output. Set to zero (default) to include all faces.
    &#34;&#34;&#34;

    def __init__(self, cutoff=0):
        self.mtcnn = import_module(&#34;mtcnn.mtcnn&#34;)
        self.cutoff = cutoff
        self._mt = self.mtcnn.MTCNN(min_face_size=20)

    def detect(self, img):
        &#34;&#34;&#34;Detect faces in an image.

        Args:
            img (numpy array): A single image stored as a three-dimensional
                numpy array.

        Returns:
            A list of dictionaries where each dictionary represents a detected
            face. Keys include the bounding box (top, left, bottom, right) as
            well as a confidence score.
        &#34;&#34;&#34;
        dets = self._mt.detect_faces(img)

        if not dets:
            return

        faces = {
            &#34;top&#34;: [],
            &#34;right&#34;: [],
            &#34;bottom&#34;: [],
            &#34;left&#34;: [],
            &#34;confidence&#34;: [],
        }
        for det in dets:
            if det[&#34;confidence&#34;] &gt;= self.cutoff:
                bbox = _trim_bbox(
                    (
                        det[&#34;box&#34;][1],
                        det[&#34;box&#34;][0] + det[&#34;box&#34;][2],
                        det[&#34;box&#34;][1] + det[&#34;box&#34;][3],
                        det[&#34;box&#34;][0],
                    ),
                    img.shape,
                )
                faces[&#34;top&#34;] += [bbox[0]]
                faces[&#34;right&#34;] += [bbox[1]]
                faces[&#34;bottom&#34;] += [bbox[2]]
                faces[&#34;left&#34;] += [bbox[3]]
                faces[&#34;confidence&#34;] += [det[&#34;confidence&#34;]]

        return faces


class FaceEmbedVgg2:
    &#34;&#34;&#34;Embed faces using the VGGFace2 model.

    A face embedding with state-of-the-art results, particularly suitable when
    there are small or non-forward-facing examples in the dataset.
    &#34;&#34;&#34;

    def __init__(self):
        from keras.models import load_model
        from keras.utils import get_file
        from keras import backend as K

        ssl._create_default_https_context = ssl._create_unverified_context
        mloc = get_file(
            &#34;vggface2-resnet50.h5&#34;,
            origin=&#34;https://github.com/distant-viewing/dvt/&#34;
            &#34;releases/download/0.0.1/&#34;
            &#34;vggface2-resnet50.h5&#34;,
        )
        self._model = load_model(mloc)
        self._iformat = K.image_data_format()

    def embed(self, img, top, right, bottom, left):
        &#34;&#34;&#34;Embed detected faces in an image.&#34;&#34;&#34;

        iscale = self._proc_image(
            _sub_image(
                img=img,
                top=top,
                right=right,
                bottom=bottom,
                left=left,
                fct=1.3,
                output_shape=(224, 224),
            )
        )

        return self._model.predict(iscale)[0, 0, 0, :]

    def _proc_image(self, iscale):
        iscale = float32(iscale)
        iscale = expand_dims(iscale, axis=0)

        if self._iformat == &#34;channels_first&#34;:  # pragma: no cover
            iscale = iscale[:, ::-1, ...]
            iscale[:, 0, :, :] -= 91.4953
            iscale[:, 1, :, :] -= 103.8827
            iscale[:, 2, :, :] -= 131.0912
        else:
            iscale = iscale[..., ::-1]
            iscale[..., 0] -= 91.4953
            iscale[..., 1] -= 103.8827
            iscale[..., 2] -= 131.0912

        return iscale


class EmbedAnnotator(ImageAnnotator):
    &#34;&#34;&#34;Annotator for embedding frames into an ambient space.&#34;&#34;&#34;

    def __init__(self, embedding):
        self.embedding = embedding

    def annotate_image(self, img):
        &#34;&#34;&#34;Annotate the images.&#34;&#34;&#34;

        obj = self.embedding.embed(img)

        return {&#34;embed&#34;: obj}


class EmbedImageKeras:
    &#34;&#34;&#34;A generic class for applying an embedding to frames.

    Applies a keras model to a batch of frames. The input of the model is
    assumed to be an image with three channels. The class automatically
    handles resizing the images to the required input shape.

    Attributes:
        model: A keras model to apply to the frames.
        preprocess_input: An optional function to preprocess the images. Set to
            None (the default) to not apply any preprocessing.
        outlayer: Name of the output layer. Set to None (the default) to use
            the final layer predictions as the embedding.
    &#34;&#34;&#34;

    def __init__(self, model, preprocess_input=None, outlayer=None):
        from keras.models import Model

        if outlayer is not None:
            model = Model(
                inputs=model.input, outputs=model.get_layer(outlayer).output
            )

        self.input_shape = (model.input_shape[1], model.input_shape[2])
        self.model = model
        self.preprocess_input = preprocess_input
        super().__init__()

    def embed(self, img):
        &#34;&#34;&#34;Embed a batch of images.

        Args:
            img: A four dimensional numpy array to embed using the keras model.

        Returns:
            A numpy array, with a first dimension matching the first dimension
            of the input image.
        &#34;&#34;&#34;

        img = cvtColor(img, COLOR_BGR2RGB)
        img = resize(img, self.input_shape)
        img = expand_dims(img, axis=0)

        # process the inputs image
        if self.preprocess_input:
            img = self.preprocess_input(img)

        # produce embeddings
        embed = self.model.predict(img)

        return {&#34;embed&#34;: embed}


class EmbedImageKerasResNet50(EmbedImageKeras):
    &#34;&#34;&#34;Example embedding using ResNet50.

    Provides an example of how to use an embedding annotator and provides
    easy access to one of the most popular models for computing image
    similarity metrics in an embedding space. See the (very minimal) source
    code for how to extend this function to other pre-built keras models.

    Attributes:
        model: The ResNet-50 model, tuned to produce the penultimate layer as
            an output.
        preprocess_input: Default processing function for an image provided as
            an array in RGB format.
    &#34;&#34;&#34;

    def __init__(self):
        import keras.applications.resnet50

        ssl._create_default_https_context = ssl._create_unverified_context
        model = keras.applications.resnet50.ResNet50(weights=&#34;imagenet&#34;)
        ppobj = keras.applications.resnet50.preprocess_input

        super().__init__(model, ppobj, outlayer=&#34;avg_pool&#34;)


def _sub_image(img, top, right, bottom, left, fct=1, output_shape=None):
    &#34;&#34;&#34;Take a subset of an input image and return a (resized) subimage.

    Args:
        img (numpy array): Image stored as a three-dimensional image (rows,
            columns, and color channels).
        top (int): Top coordinate of the new image.
        right (int): Right coordinate of the new image.
        bottom (int): Bottom coordinate of the new image.
        left (int): Left coordinate of the new image.
        fct (float): Percentage to expand the bounding box by. Defaults to
            1, using the input coordinates as given.
        output_shape (tuple): Size to scale the output image, in pixels. Set
            to None (default) to keep the native resolution.

    Returns:
        A three-dimensional numpy array describing the new image.
    &#34;&#34;&#34;

    # convert to center, height and width:
    center = [int((top + bottom) / 2), int((left + right) / 2)]
    height = int((bottom - top) / 2 * fct)
    width = int((right - left) / 2 * fct)
    box = [
        center[0] - height,
        center[0] + height,
        center[1] - width,
        center[1] + width,
    ]

    # crop the image as an array
    box[0] = max(0, box[0])
    box[2] = max(0, box[2])
    box[1] = min(img.shape[0], box[1])
    box[3] = min(img.shape[1], box[3])
    crop_img = img[box[0] : box[1], box[2] : box[3], :]

    if output_shape:
        img_scaled = resize(crop_img, output_shape)
    else:
        img_scaled = crop_img

    return uint8(img_scaled)


def _trim_bbox(css, image_shape):
    &#34;&#34;&#34;Given a bounding box and image size, returns a new trimmed bounding box.

    Some algorithms produce bounding boxes that extend over the edges of the
    source image. This function takes such a box and returns a new bounding
    box trimmed to the source.

    Args:
        css (array): An array of dimension four.
        image_shape (array): An array of dimension two.

    Returns:
        An updated bounding box trimmed to the extend of the image.
    &#34;&#34;&#34;
    return (
        max(css[0], 0),
        min(css[1], image_shape[1]),
        min(css[2], image_shape[0]),
        max(css[3], 0),
    )</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="dvt.keras.cvtColor"><code class="name flex">
<span>def <span class="ident">cvtColor</span></span>(<span>src, code, dst, dstCn)</span>
</code></dt>
<dd>
<div class="desc"><p>cvtColor(src, code[, dst[, dstCn]]) -&gt; dst
.
@brief Converts an image from one color space to another.
. <br>
.
The function converts an input image from one color space to another. In case of a transformation
.
to-from RGB color space, the order of the channels should be specified explicitly (RGB or BGR). Note
.
that the default color format in OpenCV is often referred to as RGB but it is actually BGR (the
.
bytes are reversed). So the first byte in a standard (24-bit) color image will be an 8-bit Blue
.
component, the second byte will be Green, and the third byte will be Red. The fourth, fifth, and
.
sixth bytes would then be the second pixel (Blue, then Green, then Red), and so on.
. <br>
.
The conventional ranges for R, G, and B channel values are:
.
-
0 to 255 for CV_8U images
.
-
0 to 65535 for CV_16U images
.
-
0 to 1 for CV_32F images
. <br>
.
In case of linear transformations, the range does not matter. But in case of a non-linear
.
transformation, an input RGB image should be normalized to the proper value range to get the correct
.
results, for example, for RGB \f$\rightarrow\f$ L*u*v* transformation. For example, if you have a
.
32-bit floating-point image directly converted from an 8-bit image without any scaling, then it will
.
have the 0..255 value range instead of 0..1 assumed by the function. So, before calling #cvtColor ,
.
you need first to scale the image down:
.
@code
.
img *= 1./255;
.
cvtColor(img, img, COLOR_BGR2Luv);
.
@endcode
.
If you use #cvtColor with 8-bit images, the conversion will have some information lost. For many
.
applications, this will not be noticeable but it is recommended to use 32-bit images in applications
.
that need the full range of colors or that convert an image before an operation and then convert
.
back.
. <br>
.
If conversion adds the alpha channel, its value will set to the maximum of corresponding channel
.
range: 255 for CV_8U, 65535 for CV_16U, 1 for CV_32F.
. <br>
.
@param src input image: 8-bit unsigned, 16-bit unsigned ( CV_16UC&hellip; ), or single-precision
.
floating-point.
.
@param dst output image of the same size and depth as src.
.
@param code color space conversion code (see #ColorConversionCodes).
.
@param dstCn number of channels in the destination image; if the parameter is 0, the number of the
.
channels is derived automatically from src and code.
. <br>
.
@see @ref imgproc_color_conversions</p></div>
</dd>
<dt id="dvt.keras.resize"><code class="name flex">
<span>def <span class="ident">resize</span></span>(<span>src, dsize, dst, fx, fy, interpolation)</span>
</code></dt>
<dd>
<div class="desc"><p>resize(src, dsize[, dst[, fx[, fy[, interpolation]]]]) -&gt; dst
.
@brief Resizes an image.
. <br>
.
The function resize resizes the image src down to or up to the specified size. Note that the
.
initial dst type or size are not taken into account. Instead, the size and type are derived from
.
the <code>src</code>,<code>dsize</code>,<code>fx</code>, and <code>fy</code>. If you want to resize src so that it fits the pre-created dst,
.
you may call the function as follows:
.
@code
.
// explicitly specify dsize=dst.size(); fx and fy will be computed from that.
.
resize(src, dst, dst.size(), 0, 0, interpolation);
.
@endcode
.
If you want to decimate the image by factor of 2 in each direction, you can call the function this
.
way:
.
@code
.
// specify fx and fy and let the function compute the destination image size.
.
resize(src, dst, Size(), 0.5, 0.5, interpolation);
.
@endcode
.
To shrink an image, it will generally look best with #INTER_AREA interpolation, whereas to
.
enlarge an image, it will generally look best with c#INTER_CUBIC (slow) or #INTER_LINEAR
.
(faster but still looks OK).
. <br>
.
@param src input image.
.
@param dst output image; it has the size dsize (when it is non-zero) or the size computed from
.
src.size(), fx, and fy; the type of dst is the same as of src.
.
@param dsize output image size; if it equals zero, it is computed as:
.
\f[\texttt{dsize = Size(round(fx<em>src.cols), round(fy</em>src.rows))}\f]
.
Either dsize or both fx and fy must be non-zero.
.
@param fx scale factor along the horizontal axis; when it equals 0, it is computed as
.
\f[\texttt{(double)dsize.width/src.cols}\f]
.
@param fy scale factor along the vertical axis; when it equals 0, it is computed as
.
\f[\texttt{(double)dsize.height/src.rows}\f]
.
@param interpolation interpolation method, see #InterpolationFlags
. <br>
.
@sa
warpAffine, warpPerspective, remap</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="dvt.keras.EmbedAnnotator"><code class="flex name class">
<span>class <span class="ident">EmbedAnnotator</span></span>
<span>(</span><span>embedding)</span>
</code></dt>
<dd>
<div class="desc"><p>Annotator for embedding frames into an ambient space.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EmbedAnnotator(ImageAnnotator):
    &#34;&#34;&#34;Annotator for embedding frames into an ambient space.&#34;&#34;&#34;

    def __init__(self, embedding):
        self.embedding = embedding

    def annotate_image(self, img):
        &#34;&#34;&#34;Annotate the images.&#34;&#34;&#34;

        obj = self.embedding.embed(img)

        return {&#34;embed&#34;: obj}</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="dvt.abstract.ImageAnnotator" href="abstract.html#dvt.abstract.ImageAnnotator">ImageAnnotator</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="dvt.keras.EmbedAnnotator.annotate_image"><code class="name flex">
<span>def <span class="ident">annotate_image</span></span>(<span>self, img)</span>
</code></dt>
<dd>
<div class="desc"><p>Annotate the images.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def annotate_image(self, img):
    &#34;&#34;&#34;Annotate the images.&#34;&#34;&#34;

    obj = self.embedding.embed(img)

    return {&#34;embed&#34;: obj}</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="dvt.abstract.ImageAnnotator" href="abstract.html#dvt.abstract.ImageAnnotator">ImageAnnotator</a></b></code>:
<ul class="hlist">
<li><code><a title="dvt.abstract.ImageAnnotator.annotate" href="abstract.html#dvt.abstract.ImageAnnotator.annotate">annotate</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="dvt.keras.EmbedImageKeras"><code class="flex name class">
<span>class <span class="ident">EmbedImageKeras</span></span>
<span>(</span><span>model, preprocess_input=None, outlayer=None)</span>
</code></dt>
<dd>
<div class="desc"><p>A generic class for applying an embedding to frames.</p>
<p>Applies a keras model to a batch of frames. The input of the model is
assumed to be an image with three channels. The class automatically
handles resizing the images to the required input shape.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>model</code></strong></dt>
<dd>A keras model to apply to the frames.</dd>
<dt><strong><code>preprocess_input</code></strong></dt>
<dd>An optional function to preprocess the images. Set to
None (the default) to not apply any preprocessing.</dd>
<dt><strong><code>outlayer</code></strong></dt>
<dd>Name of the output layer. Set to None (the default) to use
the final layer predictions as the embedding.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EmbedImageKeras:
    &#34;&#34;&#34;A generic class for applying an embedding to frames.

    Applies a keras model to a batch of frames. The input of the model is
    assumed to be an image with three channels. The class automatically
    handles resizing the images to the required input shape.

    Attributes:
        model: A keras model to apply to the frames.
        preprocess_input: An optional function to preprocess the images. Set to
            None (the default) to not apply any preprocessing.
        outlayer: Name of the output layer. Set to None (the default) to use
            the final layer predictions as the embedding.
    &#34;&#34;&#34;

    def __init__(self, model, preprocess_input=None, outlayer=None):
        from keras.models import Model

        if outlayer is not None:
            model = Model(
                inputs=model.input, outputs=model.get_layer(outlayer).output
            )

        self.input_shape = (model.input_shape[1], model.input_shape[2])
        self.model = model
        self.preprocess_input = preprocess_input
        super().__init__()

    def embed(self, img):
        &#34;&#34;&#34;Embed a batch of images.

        Args:
            img: A four dimensional numpy array to embed using the keras model.

        Returns:
            A numpy array, with a first dimension matching the first dimension
            of the input image.
        &#34;&#34;&#34;

        img = cvtColor(img, COLOR_BGR2RGB)
        img = resize(img, self.input_shape)
        img = expand_dims(img, axis=0)

        # process the inputs image
        if self.preprocess_input:
            img = self.preprocess_input(img)

        # produce embeddings
        embed = self.model.predict(img)

        return {&#34;embed&#34;: embed}</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="dvt.keras.EmbedImageKerasResNet50" href="#dvt.keras.EmbedImageKerasResNet50">EmbedImageKerasResNet50</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="dvt.keras.EmbedImageKeras.embed"><code class="name flex">
<span>def <span class="ident">embed</span></span>(<span>self, img)</span>
</code></dt>
<dd>
<div class="desc"><p>Embed a batch of images.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>img</code></strong></dt>
<dd>A four dimensional numpy array to embed using the keras model.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A numpy array, with a first dimension matching the first dimension
of the input image.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def embed(self, img):
    &#34;&#34;&#34;Embed a batch of images.

    Args:
        img: A four dimensional numpy array to embed using the keras model.

    Returns:
        A numpy array, with a first dimension matching the first dimension
        of the input image.
    &#34;&#34;&#34;

    img = cvtColor(img, COLOR_BGR2RGB)
    img = resize(img, self.input_shape)
    img = expand_dims(img, axis=0)

    # process the inputs image
    if self.preprocess_input:
        img = self.preprocess_input(img)

    # produce embeddings
    embed = self.model.predict(img)

    return {&#34;embed&#34;: embed}</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="dvt.keras.EmbedImageKerasResNet50"><code class="flex name class">
<span>class <span class="ident">EmbedImageKerasResNet50</span></span>
</code></dt>
<dd>
<div class="desc"><p>Example embedding using ResNet50.</p>
<p>Provides an example of how to use an embedding annotator and provides
easy access to one of the most popular models for computing image
similarity metrics in an embedding space. See the (very minimal) source
code for how to extend this function to other pre-built keras models.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>model</code></strong></dt>
<dd>The ResNet-50 model, tuned to produce the penultimate layer as
an output.</dd>
<dt><strong><code>preprocess_input</code></strong></dt>
<dd>Default processing function for an image provided as
an array in RGB format.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EmbedImageKerasResNet50(EmbedImageKeras):
    &#34;&#34;&#34;Example embedding using ResNet50.

    Provides an example of how to use an embedding annotator and provides
    easy access to one of the most popular models for computing image
    similarity metrics in an embedding space. See the (very minimal) source
    code for how to extend this function to other pre-built keras models.

    Attributes:
        model: The ResNet-50 model, tuned to produce the penultimate layer as
            an output.
        preprocess_input: Default processing function for an image provided as
            an array in RGB format.
    &#34;&#34;&#34;

    def __init__(self):
        import keras.applications.resnet50

        ssl._create_default_https_context = ssl._create_unverified_context
        model = keras.applications.resnet50.ResNet50(weights=&#34;imagenet&#34;)
        ppobj = keras.applications.resnet50.preprocess_input

        super().__init__(model, ppobj, outlayer=&#34;avg_pool&#34;)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="dvt.keras.EmbedImageKeras" href="#dvt.keras.EmbedImageKeras">EmbedImageKeras</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="dvt.keras.EmbedImageKeras" href="#dvt.keras.EmbedImageKeras">EmbedImageKeras</a></b></code>:
<ul class="hlist">
<li><code><a title="dvt.keras.EmbedImageKeras.embed" href="#dvt.keras.EmbedImageKeras.embed">embed</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="dvt.keras.FaceAnnotator"><code class="flex name class">
<span>class <span class="ident">FaceAnnotator</span></span>
<span>(</span><span>detector, embedding=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Annotator for detecting faces and embedding them as a face vector.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FaceAnnotator(ImageAnnotator):
    &#34;&#34;&#34;Annotator for detecting faces and embedding them as a face vector.&#34;&#34;&#34;

    def __init__(self, detector, embedding=None):
        self.detector = detector
        self.embedding = embedding

    def annotate_image(self, img):
        &#34;&#34;&#34;Annotate the batch of frames with the face annotator.&#34;&#34;&#34;

        img_rgb = cvtColor(img, COLOR_BGR2RGB)
        f_faces = self.detector.detect(img_rgb)

        if self.embedding is not None and f_faces is not None:
            n_face = len(f_faces[&#34;top&#34;])
            f_faces[&#34;embed&#34;] = []
            for i in range(n_face):
                embed = self.embedding.embed(
                    img_rgb,
                    top=f_faces[&#34;top&#34;][i],
                    right=f_faces[&#34;right&#34;][i],
                    bottom=f_faces[&#34;bottom&#34;][i],
                    left=f_faces[&#34;left&#34;][i],
                )
                f_faces[&#34;embed&#34;] += [embed]

        if not f_faces:
            f_faces = DataFrame(
                columns = [&#34;top&#34;, &#34;right&#34;, &#34;bottom&#34;, &#34;left&#34;, &#34;confidence&#34;]
            )

        return {&#34;face&#34;: f_faces}</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="dvt.abstract.ImageAnnotator" href="abstract.html#dvt.abstract.ImageAnnotator">ImageAnnotator</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="dvt.keras.FaceAnnotator.annotate_image"><code class="name flex">
<span>def <span class="ident">annotate_image</span></span>(<span>self, img)</span>
</code></dt>
<dd>
<div class="desc"><p>Annotate the batch of frames with the face annotator.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def annotate_image(self, img):
    &#34;&#34;&#34;Annotate the batch of frames with the face annotator.&#34;&#34;&#34;

    img_rgb = cvtColor(img, COLOR_BGR2RGB)
    f_faces = self.detector.detect(img_rgb)

    if self.embedding is not None and f_faces is not None:
        n_face = len(f_faces[&#34;top&#34;])
        f_faces[&#34;embed&#34;] = []
        for i in range(n_face):
            embed = self.embedding.embed(
                img_rgb,
                top=f_faces[&#34;top&#34;][i],
                right=f_faces[&#34;right&#34;][i],
                bottom=f_faces[&#34;bottom&#34;][i],
                left=f_faces[&#34;left&#34;][i],
            )
            f_faces[&#34;embed&#34;] += [embed]

    if not f_faces:
        f_faces = DataFrame(
            columns = [&#34;top&#34;, &#34;right&#34;, &#34;bottom&#34;, &#34;left&#34;, &#34;confidence&#34;]
        )

    return {&#34;face&#34;: f_faces}</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="dvt.abstract.ImageAnnotator" href="abstract.html#dvt.abstract.ImageAnnotator">ImageAnnotator</a></b></code>:
<ul class="hlist">
<li><code><a title="dvt.abstract.ImageAnnotator.annotate" href="abstract.html#dvt.abstract.ImageAnnotator.annotate">annotate</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="dvt.keras.FaceDetectMtcnn"><code class="flex name class">
<span>class <span class="ident">FaceDetectMtcnn</span></span>
<span>(</span><span>cutoff=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Detect faces using the Multi-task Cascaded CNN model.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>cutoff</code></strong> :&ensp;<code>float</code></dt>
<dd>A cutoff value for which faces to include in the final
output. Set to zero (default) to include all faces.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FaceDetectMtcnn:
    &#34;&#34;&#34;Detect faces using the Multi-task Cascaded CNN model.

    Attributes:
        cutoff (float): A cutoff value for which faces to include in the final
            output. Set to zero (default) to include all faces.
    &#34;&#34;&#34;

    def __init__(self, cutoff=0):
        self.mtcnn = import_module(&#34;mtcnn.mtcnn&#34;)
        self.cutoff = cutoff
        self._mt = self.mtcnn.MTCNN(min_face_size=20)

    def detect(self, img):
        &#34;&#34;&#34;Detect faces in an image.

        Args:
            img (numpy array): A single image stored as a three-dimensional
                numpy array.

        Returns:
            A list of dictionaries where each dictionary represents a detected
            face. Keys include the bounding box (top, left, bottom, right) as
            well as a confidence score.
        &#34;&#34;&#34;
        dets = self._mt.detect_faces(img)

        if not dets:
            return

        faces = {
            &#34;top&#34;: [],
            &#34;right&#34;: [],
            &#34;bottom&#34;: [],
            &#34;left&#34;: [],
            &#34;confidence&#34;: [],
        }
        for det in dets:
            if det[&#34;confidence&#34;] &gt;= self.cutoff:
                bbox = _trim_bbox(
                    (
                        det[&#34;box&#34;][1],
                        det[&#34;box&#34;][0] + det[&#34;box&#34;][2],
                        det[&#34;box&#34;][1] + det[&#34;box&#34;][3],
                        det[&#34;box&#34;][0],
                    ),
                    img.shape,
                )
                faces[&#34;top&#34;] += [bbox[0]]
                faces[&#34;right&#34;] += [bbox[1]]
                faces[&#34;bottom&#34;] += [bbox[2]]
                faces[&#34;left&#34;] += [bbox[3]]
                faces[&#34;confidence&#34;] += [det[&#34;confidence&#34;]]

        return faces</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="dvt.keras.FaceDetectMtcnn.detect"><code class="name flex">
<span>def <span class="ident">detect</span></span>(<span>self, img)</span>
</code></dt>
<dd>
<div class="desc"><p>Detect faces in an image.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>img</code></strong> :&ensp;<code>numpy array</code></dt>
<dd>A single image stored as a three-dimensional
numpy array.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A list of dictionaries where each dictionary represents a detected
face. Keys include the bounding box (top, left, bottom, right) as
well as a confidence score.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def detect(self, img):
    &#34;&#34;&#34;Detect faces in an image.

    Args:
        img (numpy array): A single image stored as a three-dimensional
            numpy array.

    Returns:
        A list of dictionaries where each dictionary represents a detected
        face. Keys include the bounding box (top, left, bottom, right) as
        well as a confidence score.
    &#34;&#34;&#34;
    dets = self._mt.detect_faces(img)

    if not dets:
        return

    faces = {
        &#34;top&#34;: [],
        &#34;right&#34;: [],
        &#34;bottom&#34;: [],
        &#34;left&#34;: [],
        &#34;confidence&#34;: [],
    }
    for det in dets:
        if det[&#34;confidence&#34;] &gt;= self.cutoff:
            bbox = _trim_bbox(
                (
                    det[&#34;box&#34;][1],
                    det[&#34;box&#34;][0] + det[&#34;box&#34;][2],
                    det[&#34;box&#34;][1] + det[&#34;box&#34;][3],
                    det[&#34;box&#34;][0],
                ),
                img.shape,
            )
            faces[&#34;top&#34;] += [bbox[0]]
            faces[&#34;right&#34;] += [bbox[1]]
            faces[&#34;bottom&#34;] += [bbox[2]]
            faces[&#34;left&#34;] += [bbox[3]]
            faces[&#34;confidence&#34;] += [det[&#34;confidence&#34;]]

    return faces</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="dvt.keras.FaceEmbedVgg2"><code class="flex name class">
<span>class <span class="ident">FaceEmbedVgg2</span></span>
</code></dt>
<dd>
<div class="desc"><p>Embed faces using the VGGFace2 model.</p>
<p>A face embedding with state-of-the-art results, particularly suitable when
there are small or non-forward-facing examples in the dataset.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FaceEmbedVgg2:
    &#34;&#34;&#34;Embed faces using the VGGFace2 model.

    A face embedding with state-of-the-art results, particularly suitable when
    there are small or non-forward-facing examples in the dataset.
    &#34;&#34;&#34;

    def __init__(self):
        from keras.models import load_model
        from keras.utils import get_file
        from keras import backend as K

        ssl._create_default_https_context = ssl._create_unverified_context
        mloc = get_file(
            &#34;vggface2-resnet50.h5&#34;,
            origin=&#34;https://github.com/distant-viewing/dvt/&#34;
            &#34;releases/download/0.0.1/&#34;
            &#34;vggface2-resnet50.h5&#34;,
        )
        self._model = load_model(mloc)
        self._iformat = K.image_data_format()

    def embed(self, img, top, right, bottom, left):
        &#34;&#34;&#34;Embed detected faces in an image.&#34;&#34;&#34;

        iscale = self._proc_image(
            _sub_image(
                img=img,
                top=top,
                right=right,
                bottom=bottom,
                left=left,
                fct=1.3,
                output_shape=(224, 224),
            )
        )

        return self._model.predict(iscale)[0, 0, 0, :]

    def _proc_image(self, iscale):
        iscale = float32(iscale)
        iscale = expand_dims(iscale, axis=0)

        if self._iformat == &#34;channels_first&#34;:  # pragma: no cover
            iscale = iscale[:, ::-1, ...]
            iscale[:, 0, :, :] -= 91.4953
            iscale[:, 1, :, :] -= 103.8827
            iscale[:, 2, :, :] -= 131.0912
        else:
            iscale = iscale[..., ::-1]
            iscale[..., 0] -= 91.4953
            iscale[..., 1] -= 103.8827
            iscale[..., 2] -= 131.0912

        return iscale</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="dvt.keras.FaceEmbedVgg2.embed"><code class="name flex">
<span>def <span class="ident">embed</span></span>(<span>self, img, top, right, bottom, left)</span>
</code></dt>
<dd>
<div class="desc"><p>Embed detected faces in an image.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def embed(self, img, top, right, bottom, left):
    &#34;&#34;&#34;Embed detected faces in an image.&#34;&#34;&#34;

    iscale = self._proc_image(
        _sub_image(
            img=img,
            top=top,
            right=right,
            bottom=bottom,
            left=left,
            fct=1.3,
            output_shape=(224, 224),
        )
    )

    return self._model.predict(iscale)[0, 0, 0, :]</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<header>
<a class="homelink" rel="home" title="pdoc Home" href="https://www.distantviewing.org/">
<img src="https://www.distantviewing.org/img/tv.png" alt="" height="50">
</a>
</header>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="dvt" href="index.html">dvt</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="dvt.keras.cvtColor" href="#dvt.keras.cvtColor">cvtColor</a></code></li>
<li><code><a title="dvt.keras.resize" href="#dvt.keras.resize">resize</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="dvt.keras.EmbedAnnotator" href="#dvt.keras.EmbedAnnotator">EmbedAnnotator</a></code></h4>
<ul class="">
<li><code><a title="dvt.keras.EmbedAnnotator.annotate_image" href="#dvt.keras.EmbedAnnotator.annotate_image">annotate_image</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="dvt.keras.EmbedImageKeras" href="#dvt.keras.EmbedImageKeras">EmbedImageKeras</a></code></h4>
<ul class="">
<li><code><a title="dvt.keras.EmbedImageKeras.embed" href="#dvt.keras.EmbedImageKeras.embed">embed</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="dvt.keras.EmbedImageKerasResNet50" href="#dvt.keras.EmbedImageKerasResNet50">EmbedImageKerasResNet50</a></code></h4>
</li>
<li>
<h4><code><a title="dvt.keras.FaceAnnotator" href="#dvt.keras.FaceAnnotator">FaceAnnotator</a></code></h4>
<ul class="">
<li><code><a title="dvt.keras.FaceAnnotator.annotate_image" href="#dvt.keras.FaceAnnotator.annotate_image">annotate_image</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="dvt.keras.FaceDetectMtcnn" href="#dvt.keras.FaceDetectMtcnn">FaceDetectMtcnn</a></code></h4>
<ul class="">
<li><code><a title="dvt.keras.FaceDetectMtcnn.detect" href="#dvt.keras.FaceDetectMtcnn.detect">detect</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="dvt.keras.FaceEmbedVgg2" href="#dvt.keras.FaceEmbedVgg2">FaceEmbedVgg2</a></code></h4>
<ul class="">
<li><code><a title="dvt.keras.FaceEmbedVgg2.embed" href="#dvt.keras.FaceEmbedVgg2.embed">embed</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p><span style="color:#ddd">&#21328;</span></p>
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>